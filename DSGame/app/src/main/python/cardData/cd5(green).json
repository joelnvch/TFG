[
   {
      "name": "Akaike Information Criterion",
      "desc": "An estimator of the relative quality of statistical models for a data set. Given a collection of models, AIC estimates the quality of each model, relative to each of the other models. ",
      "costs": {
         "blue": 0,
         "gray": 0,
         "yellow": 2,
         "red": 2,
         "green": 3
      },
      "letters": [
         "M"
      ],
      "value": 1,
      "card_color": "green"
   },
   {
      "name": "Crossvalidation",
      "desc": "Model validation method to evaluate the results of a statistical analysis and ensure that they are independent on the partition between training and test data.",
      "costs": {
         "blue": 0,
         "gray": 0,
         "yellow": 3,
         "red": 0,
         "green": 3
      },
      "letters": [
         "A"
      ],
      "value": 2,
      "card_color": "green"
   },
   {
      "name": "ROC",
      "desc": "Reciever OperatingCharacteristic: False Positive Rate vs. True Positive Rate for all probability thresholds of a binary classifier. ",
      "costs": {
         "blue": 1,
         "gray": 1,
         "yellow": 1,
         "red": 1,
         "green": 3
      },
      "letters": [
         "L"
      ],
      "value": 2,
      "card_color": "green"
   },
   {
      "name": "Model Goodness",
      "desc": "Quantifying business value and converting typical machine learning performance metrics (like precision, recall, MSE, etc.) to business metrics. ",
      "costs": {
         "blue": 0,
         "gray": 0,
         "yellow": 3,
         "red": 2,
         "green": 2
      },
      "letters": [
         "V"
      ],
      "value": 3,
      "card_color": "green"
   },
   {
      "name": "Model Stability",
      "desc": "Stability analysis enables to determine how the input variations are going to impact the output of the model. ",
      "costs": {
         "blue": 2,
         "gray": 0,
         "yellow": 3,
         "red": 1,
         "green": 1
      },
      "letters": [
         "M"
      ],
      "value": 1,
      "card_color": "green"
   },
   {
      "name": "Bayes Error",
      "desc": "The smallest possible error. Under the true population distribution, true prediction error due to inherent noise or limitations in the data. ",
      "costs": {
         "blue": 1,
         "gray": 3,
         "yellow": 0,
         "red": 2,
         "green": 2
      },
      "letters": [],
      "value": 2,
      "card_color": "green"
   },
   {
      "name": "Model Complexity",
      "desc": "A measure is defined to evaluate how complex a model is, for instance, the number of features or terms included in a given predictive model. ",
      "costs": {
         "blue": 0,
         "gray": 3,
         "yellow": 0,
         "red": 3,
         "green": 1
      },
      "letters": [
         "C"
      ],
      "value": 2,
      "card_color": "green"
   },
   {
      "name": "MSE",
      "desc": "Mean Squared Error, the average squared difference between the estimated values and what is estimated.",
      "costs": {
         "blue": 1,
         "gray": 0,
         "yellow": 3,
         "red": 2,
         "green": 0
      },
      "letters": [
         "L"
      ],
      "value": 2,
      "card_color": "green"
   },
   {
      "name": "Confusion Matrix",
      "desc": "Visualize the accuracy of a classifier by comparing the true and predicted classes. Off diagonal values are incorrect predictions.",
      "costs": {
         "blue": 0,
         "gray": 0,
         "yellow": 2,
         "red": 4,
         "green": 1
      },
      "letters": [
         "L"
      ],
      "value": 2,
      "card_color": "green"
   },
   {
      "name": "Accuracy",
      "desc": "A common metric in classification problems. Difference between the predicted and true values of the target of interest.",
      "costs": {
         "blue": 1,
         "gray": 2,
         "yellow": 0,
         "red": 2,
         "green": 2
      },
      "letters": [
         "L"
      ],
      "value": 1,
      "card_color": "green"
   }
]